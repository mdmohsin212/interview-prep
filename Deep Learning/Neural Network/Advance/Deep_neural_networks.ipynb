{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581c4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e5d06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP:\n",
    "    def __init__(self, layers, learning_rate=0.01, activation='relu'):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # He initialization for ReLU\n",
    "        for i in range(len(layers) - 1):\n",
    "            if activation == 'relu':\n",
    "                std = np.sqrt(2.0 / layers[i])\n",
    "            else:\n",
    "                std = np.sqrt(1.0 / layers[i])\n",
    "            w = np.random.randn(layers[i], layers[i+1]) * std\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            if self.activation == 'relu':\n",
    "                a = self.relu(z)\n",
    "            else:\n",
    "                a = 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, activations, y):\n",
    "        m = y.shape[0]\n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "        \n",
    "        error = activations[-1] - y\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            delta = error * self.relu_derivative(activations[-1])\n",
    "        else:\n",
    "            delta = error * activations[-1] * (1 - activations[-1])\n",
    "        \n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            grad_w = np.dot(activations[i].T, delta) / m\n",
    "            grad_b = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            gradients_w.insert(0, grad_w)\n",
    "            gradients_b.insert(0, grad_b)\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T)\n",
    "                if self.activation == 'relu':\n",
    "                    delta = delta * self.relu_derivative(activations[i])\n",
    "                else:\n",
    "                    delta = delta * activations[i] * (1 - activations[i])\n",
    "                    \n",
    "        return gradients_w, gradients_b\n",
    "    \n",
    "    def fit(self, x, y, epochs=1000, batch_size=32):\n",
    "        n_samples = len(x)\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                batch_x = x[batch_indices]\n",
    "                batch_y = y[batch_indices]\n",
    "            \n",
    "                activations = self.forward(batch_x)\n",
    "                grad_w, grad_b = self.backward(activations, batch_y)\n",
    "                \n",
    "                for j in range(len(self.weights)):\n",
    "                    self.weights[j] -= self.learning_rate * grad_w[j]\n",
    "                    self.biases[j] -= self.learning_rate * grad_b[j]\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                activations = self.forward(x)\n",
    "                loss = np.mean((activations[-1] - y) ** 2)\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "                \n",
    "    def predict(self, x):\n",
    "        activations = self.forward(x)\n",
    "        return activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2316c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.3499\n",
      "  Input: [0 0], Target: [0], Prediction: [0], ✓\n",
      "  Input: [0 1], Target: [1], Prediction: [1], ✓\n",
      "  Input: [1 0], Target: [1], Prediction: [1], ✓\n",
      "  Input: [1 1], Target: [0], Prediction: [0], ✓\n"
     ]
    }
   ],
   "source": [
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_xor = np.array([0, 1, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "deepmlp = DeepMLP(layers=[2, 4, 1], learning_rate=0.1)\n",
    "deepmlp.fit(X_xor, Y_xor, epochs=100)\n",
    "deepmlp_xor = deepmlp.predict(X_xor)\n",
    "\n",
    "pred_labels = (deepmlp_xor >= 0.5).astype(int) \n",
    "\n",
    "for i, (x, y_true, y_pred) in enumerate(zip(X_xor, Y_xor, pred_labels)):\n",
    "    print(f\"  Input: {x}, Target: {y_true}, Prediction: {y_pred}, {'✓' if y_true == y_pred else '✗'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
