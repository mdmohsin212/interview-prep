{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6489cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5786327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "  z1 = 1.100, a1 = 0.750\n",
      "  z2 = 0.425, y_pred = 0.605\n",
      "  Loss = 0.156\n",
      "\n",
      "Backward Pass (Gradients):\n",
      "  dLoss/dw2 = -0.142\n",
      "  dLoss/db2 = -0.189\n",
      "  dLoss/dw1 = -0.021\n",
      "  dLoss/db1 = -0.011\n",
      "\n",
      "Updated Weights:\n",
      "  w1: 0.500 -> 0.502\n",
      "  b1: 0.100 -> 0.101\n",
      "  w2: 0.300 -> 0.314\n",
      "  b2: 0.200 -> 0.219\n"
     ]
    }
   ],
   "source": [
    "def backpropagation_example():\n",
    "    # Network: x -> z1 -> a1 -> z2 -> y_pred\n",
    "    # Parameters: w1, b1, w2, b2\n",
    "    \n",
    "    # forward pass\n",
    "    x = 2.0\n",
    "    w1 = 0.5\n",
    "    b1 = 0.1\n",
    "    w2 = 0.3\n",
    "    b2 = 0.2\n",
    "    y_true = 1.0\n",
    "    \n",
    "    # Layer 1\n",
    "    z1 = x * w1 + b1\n",
    "    a1 = 1 / (1 + np.exp(-z1))\n",
    "    \n",
    "    # Layer 2 (output)\n",
    "    z2 = a1 * w2 + b2\n",
    "    y_pred = 1 / (1 + np.exp(-z2))\n",
    "    \n",
    "    # Loss : MSE\n",
    "    loss = (y_true - y_pred) ** 2\n",
    "    print(\"Forward Pass:\")\n",
    "    print(f\"  z1 = {z1:.3f}, a1 = {a1:.3f}\")\n",
    "    print(f\"  z2 = {z2:.3f}, y_pred = {y_pred:.3f}\")\n",
    "    print(f\"  Loss = {loss:.3f}\")\n",
    "    \n",
    "    # Backward pass (chain rule)\n",
    "    # Gradient w.r.t. loss\n",
    "    dloss_dypred = 2 * (y_pred - y_true)\n",
    "    \n",
    "    # Gradient w.r.t. z2\n",
    "    dypred_dz2 = y_pred * (1 - y_pred)\n",
    "    dloss_dz2 = dloss_dypred * dypred_dz2\n",
    "    \n",
    "    # Gradient w.r.t. w2\n",
    "    dz2_dw2 = a1\n",
    "    dloss_dw2 = dloss_dz2 * dz2_dw2\n",
    "    \n",
    "    # Gradient w.r.t. b2\n",
    "    dz2_db2 = 1\n",
    "    dloss_db2 = dloss_dz2 * dz2_db2\n",
    "    \n",
    "    # Gradient w.r.t. a1\n",
    "    dloss_da1 = dloss_dz2 * w2\n",
    "    \n",
    "    # Gradient w.r.t. z1\n",
    "    da1_dz1 = a1 * (1 - a1)\n",
    "    dloss_dz1 = dloss_da1 * da1_dz1\n",
    "    \n",
    "    dz1_dw1 = x\n",
    "    dloss_dw1 = dloss_dz1 * dz1_dw1\n",
    "    \n",
    "    # Gradient w.r.t. b1\n",
    "    dz1_db1 = 1\n",
    "    dloss_db1 = dloss_dz1 * dz1_db1\n",
    "    \n",
    "    print(\"\\nBackward Pass (Gradients):\")\n",
    "    print(f\"  dLoss/dw2 = {dloss_dw2:.3f}\")\n",
    "    print(f\"  dLoss/db2 = {dloss_db2:.3f}\")\n",
    "    print(f\"  dLoss/dw1 = {dloss_dw1:.3f}\")\n",
    "    print(f\"  dLoss/db1 = {dloss_db1:.3f}\")\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    w1_new = w1 - learning_rate * dloss_dw1 \n",
    "    b1_new = b1 - learning_rate * dloss_db1\n",
    "    w2_new = w2 - learning_rate * dloss_dw2\n",
    "    b2_new = b2 - learning_rate * dloss_db2\n",
    "    \n",
    "    print(\"\\nUpdated Weights:\")\n",
    "    print(f\"  w1: {w1:.3f} -> {w1_new:.3f}\")\n",
    "    print(f\"  b1: {b1:.3f} -> {b1_new:.3f}\")\n",
    "    print(f\"  w2: {w2:.3f} -> {w2_new:.3f}\")\n",
    "    print(f\"  b2: {b2:.3f} -> {b2_new:.3f}\")\n",
    "\n",
    "backpropagation_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
