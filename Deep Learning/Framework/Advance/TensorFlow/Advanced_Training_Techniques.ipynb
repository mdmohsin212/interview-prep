{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5045821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528d4e3",
   "metadata": {},
   "source": [
    "**Mixed Precision Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc751268",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b421238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model (automatically uses float16)\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax', dtype='float32')  # Output in float32\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a6d5b",
   "metadata": {},
   "source": [
    "**Gradient Accumulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0fb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulationModel(keras.Model):\n",
    "    def __init__(self, accumulation_steps=4):\n",
    "        super().__init__()\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.accumulation_counter = 0\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        if self.accumulation_counter == 0:\n",
    "            self.accumulated_gradients = [tf.zeros_like(g) for g in gradients]\n",
    "        \n",
    "        for i, grad in enumerate(gradients):\n",
    "            self.accumulated_gradients[i] += grad / self.accumulation_steps\n",
    "        \n",
    "        self.accumulation_counter += 1\n",
    "\n",
    "        if self.accumulation_counter == self.accumulation_steps:\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(self.accumulated_gradients, self.trainable_variables)\n",
    "            )\n",
    "            self.accumulation_counter = 0\n",
    "        \n",
    "        return {'loss' : loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64f5ed",
   "metadata": {},
   "source": [
    "**Learning Rate Scheduling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edae6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine annealing\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b77efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-cycle policy\n",
    "def one_cycle_lr(epoch, max_epochs):\n",
    "    if epoch < max_epochs // 2:\n",
    "        return 0.001 * (2 * epoch / max_epochs)\n",
    "    else:\n",
    "        return 0.0001 * (2 - 2 * epoch / max_epochs)\n",
    "    \n",
    "lr_callback = keras.callbacks.LearningRateScheduler(one_cycle_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
