{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oB6JEUmWD9iT"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Typical NLP Architecture**\n",
        "\n",
        "`Text → Tokenization → Embedding → RNN/LSTM/Transformer → Output`"
      ],
      "metadata": {
        "id": "aHKVfV15EI_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = [\n",
        "    \"I love machine learning and artificial intelligence.\",\n",
        "    \"TensorFlow makes building neural networks easy.\",\n",
        "    \"The weather is terrible today, I hate the rain.\",\n",
        "    \"Python is a great programming language for data science.\",\n",
        "    \"Deep learning requires a lot of computing power.\",\n",
        "    \"This movie was fantastic, the plot was amazing.\",\n",
        "    \"I did not enjoy the food at that restaurant.\"\n",
        "]"
      ],
      "metadata": {
        "id": "g_cy17ewFRLj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TextVectorization Layer**"
      ],
      "metadata": {
        "id": "kdmykIpuFpnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TextVectorization layer\n",
        "\n",
        "text_vectorizer = TextVectorization(\n",
        "    max_tokens=10000,  # Maximum vocabulary size\n",
        "    output_sequence_length=250,  # Pad/truncate to this length\n",
        "    output_mode='int'  # Return integer sequences\n",
        ")\n",
        "\n",
        "# Adapt to training data\n",
        "text_vectorizer.adapt(train_texts)\n",
        "\n",
        "\n",
        "# Convert text to numbers\n",
        "text_vectorized = text_vectorizer(train_texts)\n",
        "print(f\"Vocabulary size: {text_vectorizer.vocabulary_size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2gPhM4lELry",
        "outputId": "51fa648e-5214-4769-e9ee-5944a2df49e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Embeddings**"
      ],
      "metadata": {
        "id": "kxnHEfVdFrL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = Embedding(\n",
        "    input_dim=10000,  # Vocabulary size\n",
        "    output_dim=128,   # Embedding dimension\n",
        "    input_length=250  # Sequence length\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MW2OIz5FjYJ",
        "outputId": "f2677d86-2533-411c-f51f-d9e5282fb852"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP Models with TensorFlow**\n",
        "\n",
        "- Model 0: Baseline (Dense Layers Only):"
      ],
      "metadata": {
        "id": "ZB5Gsj5fG_HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mode_0 = keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    embedding,\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, 'sigmoid')\n",
        "])\n",
        "\n",
        "mode_0.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "dUi-UhD_G_-H"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 1: Deep Dense Model:"
      ],
      "metadata": {
        "id": "h53YTQf2HbYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    embedding,\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "2yvY_Px6HWtP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 2: LSTM (Long Short-Term Memory):"
      ],
      "metadata": {
        "id": "V6lw9u8rIEeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    embedding,\n",
        "    layers.LSTM(64, return_sequences=True), # Return sequences for stacking\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "GfWzekTAIDXC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 3: GRU (Gated Recurrent Unit):"
      ],
      "metadata": {
        "id": "xmxKh-LHIYsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    embedding,\n",
        "    layers.GRU(64, return_sequences=True), # Return sequences for stacking\n",
        "    layers.GRU(32),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "MUtX3sB-IXgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 4: Bidirectional RNN:"
      ],
      "metadata": {
        "id": "JTZBFTBWIfA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    embedding,\n",
        "    layers.Bidirectional(layers.LSTM(64)),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "gstQIi6GIhar"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 5: Conv1D for Text:"
      ],
      "metadata": {
        "id": "dueiLtk8JhAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_5 = keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    embedding,\n",
        "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "G5T5HJCaJb0Y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning for NLP with TensorFlow Hub**\n",
        "\n",
        "- Using Pre-trained Embeddings:"
      ],
      "metadata": {
        "id": "Ki7u3eaRKbPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "    dtype=tf.string,\n",
        "    trainable=False     # Freeze embeddings\n",
        ")\n",
        "\n",
        "\n",
        "# Model with pre-trained embeddings\n",
        "model_6 = keras.Sequential([\n",
        "    keras.Input(shape=[], dtype=tf.string),\n",
        "    layers.Lambda(lambda x: embedding_layer(x), output_shape=(512,)),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "WN6qa9kmKWqc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Visualizing Word Embeddings:"
      ],
      "metadata": {
        "id": "gXlScdLRLkty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embedding weights\n",
        "embedding_weights = embedding.get_weights()[0]\n",
        "\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "vocab = text_vectorizer.get_vocabulary()\n",
        "for index, word in enumerate(vocab):\n",
        "    vec = embedding_weights[index]\n",
        "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "    out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "metadata": {
        "id": "wXD5Cw42LfFp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using tf.data API for Efficient Text Processing**"
      ],
      "metadata": {
        "id": "FbOkI0MmM_W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_dataset(texts, labels, batch_size=32, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=10000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = create_text_dataset(train_texts, train_labels)\n",
        "val_dataset = create_text_dataset(val_texts, val_labels, shuffle=False)"
      ],
      "metadata": {
        "id": "1p75s4fENAb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating NLP Models**"
      ],
      "metadata": {
        "id": "0UlhrDwGOyb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_pred_probs = model.predict(test_texts)\n",
        "y_pred = tf.round(y_pred_probs)\n",
        "\n",
        "cm = confusion_matrix(test_labels, y_pred)\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "# Visualize most wrong predictions\n",
        "wrong_predictions = []\n",
        "for i, (text, true_label, pred_prob) in enumerate(zip(test_texts, test_labels, y_pred_probs)):\n",
        "    if (true_label == 1 and pred_prob < 0.5) or (true_label == 0 and pred_prob > 0.5):\n",
        "        wrong_predictions.append({\n",
        "            'text': text,\n",
        "            'true': true_label,\n",
        "            'pred': pred_prob[0]\n",
        "        })"
      ],
      "metadata": {
        "id": "rEfWIdbyOz9y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}